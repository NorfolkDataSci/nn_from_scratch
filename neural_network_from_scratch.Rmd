---
title: "NN From Scratch"
output: html_notebook
---

**Various activation funtions
```{r}
sigmoid <- function (z) {
  res <- 1 / (1 + exp(-1*z))
  return(res)
}

relu <- function (z) {
  res <- max(0,z)
  return(res)
}

tanh <- function (z) {
  res <- (exp(z) - exp(-1*z)) / (exp(z) + exp(-1*z))
}
```


**Regularization functions
```{r}
dropout <- function (layer_output) {
  z <- rbinom(nrow(layer_output), 1, 0.5)
  return(z * layer_output)
}
```

**The two parts of each node
```{r}
node <- function (w, b, x, FUN = sigmoid) {
  z <- w * x + b
  node_activation(z, FUN)
}

node_activation <- function(z, FUN){
  y <- FUN(z)
  return(y)
}

```

**Layer of a NN
```{r}
layer_node <- function (row, FUN = sigmoid, weights, bias) {
  res <- c()
  for(i in 1:length(row)) {
    ##cludge to take into account dropout being implemented, this needs to change
    if (row[i] == 0) {
      res[i] = 0
    } else {
      res[i] <- node(weights[i,], bias, as.double(row[i]), FUN)
    }
  }
  
  return(sum(res))
}

##right here is where our output changes to a 1 column dataframe, will need to be transposed to list for future layers
layer <- function (df, FUN = sigmoid, weights, bias) {
  start <- Sys.time()
  res <- matrix(ncol=1, nrow=nrow(df))
  for(i in 1:nrow(df)){
    res[i,] <- layer_node(df[i,], FUN, weights, bias)
  }
  end <- Sys.time()
  print(end - start)
  return(data.frame(res))
}
```

**Subsequent layers after layer 0
```{r}
##There is an issue with the way regularization is implemented here, needs to be done in such a way that it is applied after the return from layer.  This is currently implemented for dropout with the conditional inside for the for loop in the layer_node function
layer_after_0 <- function (df, nodes, FUN = sigmoid, weights, bias) {
  
  res <- matrix(ncol=nrow(df), nrow=nodes)
  for (i in 1:nodes) {
    df_t <- t(df)
    res[i,] <- df_t
  }
  return(layer(res, FUN, weights, bias))
}
```

**Need to implement loss and cost
```{r}

```

**Need to implement weights and bias df based on shape of input df
```{r}
weights_bias_df <- function(df) {
  return(data.frame(x1 = numeric(nrow(df) + 1)))
}
```

**Need to implement random initialization
```{r}
rand_init <- function(df, bias_start) {
  K <- nrow(df)
  res <- data.frame(rnorm(K))
  res[1,] <- bias_start
  return(res)
}
```

**Need to implement backprop
```{r}

```

**Define random df for testing
```{r echo=TRUE}
N  <- 100
df  <- data.frame(x1 = rnorm(N), x2 = rnorm(N), x3 = rnorm(N), x4 = rnorm(N), x5 = rnorm(N))
```

**Implementation of a simple forward prop, with all observations and 5 epochs
```{r}
return <- as.data.frame(matrix(ncol=5, nrow=nrow(df)))
epochs <- 5

## This is set up so that we can learn these weights and biases once the backprop function is implemented, for now I am stripping out bias and weights into seperate dfs
weights_bias <- rand_init(weights_bias_df(df), 0.05)
bias <- weights_bias[1,]
ws <- weights_bias[2:nrow(weights_bias),]

weights <- rand_init(weights)

for (j in epochs:1) {
  for (i in 1:nrow(df)) {
    layer_0 <- layer(df[i,], sigmoid, weights, bias)

    layer_0_reg <- dropout(layer_0)

    layer_1 <- layer_after_0(layer_0_reg, 100, sigmoid, weights, bias)

    layer_1_reg <- dropout(layer_1)

    layer_2 <- layer_after_0(layer_1_reg, 10, tanh, weights, bias)

    layer_3 <- layer_after_0(layer_2, 1, relu, weights, bias)
  
    return[i,j] <- layer_3
  }  
}
```





