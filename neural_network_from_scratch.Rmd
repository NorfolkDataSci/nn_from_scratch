---
title: "NN From Scratch"
output: html_notebook
---

**Various activation funtions
```{r}
sigmoid <- function (z) {
  res <- 1 / (1 + exp(-1*z))
  return(res)
}

relu <- function (z) {
  res <- max(0,z)
  return(res)
}

tanh <- function (z) {
  res <- (exp(z) - exp(-1*z)) / (exp(z) + exp(-1*z))
}
```


**Regularization functions
```{r}
dropout <- function (layer_output) {
  z <- rbinom(nrow(layer_output), 1, 0.5)
  return(z * layer_output)
}
```

**The two parts of each node
```{r}
node <- function (w, b, x, FUN = sigmoid) {
  z <- w * x + b
  node_activation(z, FUN)
}

node_activation <- function(z, FUN){
  y <- FUN(z)
  return(y)
}
```

**Layer of a NN
```{r}
layer_node <- function (row, FUN = sigmoid, weights, bias) {
  res <- c()
  for(i in 1:length(row)) {
    res[i] <- node(weights[i], bias, as.double(row[i]), FUN)
  }
  
  return(sum(res))
}

##right here is where our output changes to a 1 column dataframe, will need to be transposed to list for future layers
layer <- function (df, FUN = sigmoid, weights, bias) {
  res <- matrix(ncol=1, nrow=nrow(df))
  for(i in 1:nrow(df)){
    res[i,] <- layer_node(df[i,], FUN, weights, bias)
  }
  return(data.frame(res))
}
```

**Subsequent layers after layer 0
```{r}
layer_after_0 <- function (df, nodes, FUN = sigmoid, weights, bias) {
  
  res <- matrix(ncol=nrow(df), nrow=nodes)
  for (i in 1:nodes) {
    df_t <- t(df)
    res[i,] <- df_t
  }
  return(layer(res, FUN, weights, bias))
}
```

**Need to implement loss and cost
```{r}
loss <- function(y_hat, y){
  y_hat <- as.data.frame(return[,c(1)])
  y <- df_y
  for (i in 1:nrow(y_hat)) {
   res <- y[1,]*log(y_hat[1,])+(1-y[1,])*log(1-y_hat[1,])
  }
  return(res)
}

cost <- function(l){
  res <- 1/nrow(l)*sum(l)
  return(res)
}
```

**Need to implement weights and bias df based on shape of input df
```{r}
weights_bias_df <- function(df) {
  return(data.frame(x1 = numeric(nrow(df) + 1)))
}
```

**Need to implement random initialization
```{r}
rand_init <- function(df, bias_start) {
  K <- nrow(df)
  res <- data.frame(rnorm(K))
  res[1,] <- bias_start
  return(res)
}
```

**Need to implement backprop
```{r}

```


```{r}
update <- function(weight, bias, change) {
  
}
```

**Get data set
```{r echo=TRUE}
data("iris")

iris_sub <- iris[ which(iris$Species == "setosa" | iris$Species == "virginica"), ]

df_x  <- iris_sub[,(1:4)]

df_y <- as.data.frame(iris_sub[,5])
```

**Implementation of a simple forward prop, with all observations and 5 epochs
```{r}
return <- as.data.frame(matrix(ncol=5, nrow=nrow(df_x)))
epochs <- 5

## This is set up so that we can learn these weights and biases once the backprop function is implemented, for now I am stripping out bias and weights into seperate dfs
weights_bias <- rand_init(weights_bias_df(df_x), 0.05)
bias <- weights_bias[1,]
weights <- weights_bias[2:nrow(weights_bias),]

for (j in epochs:1) {
  for (i in 1:nrow(df_x)) {
    layer_0 <- layer(df_x[i,], sigmoid, weights, bias)
  
    layer_1 <- layer_after_0(layer_0, 100, sigmoid, weights, bias)

    layer_1_reg <- dropout(layer_1)

    layer_2 <- layer_after_0(layer_1_reg, 10, tanh, weights, bias)
    
    layer_2_reg <- dropout(layer_2)

    layer_3 <- layer_after_0(layer_2_reg, 1, relu, weights, bias)
  
    return[i,j] <- layer_3
  }
}
```





