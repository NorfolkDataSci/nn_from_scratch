---
title: "NN From Scratch"
output: html_notebook
---

**Get data set
```{r echo=TRUE}
data("iris")

iris_sub <- iris[ which(iris$Species == "setosa" | iris$Species == "virginica"), ]

iris_sub <- iris_sub[sample(nrow(iris_sub)),]

smp_size <- floor(0.75 * nrow(iris_sub))
set.seed(123)
train_ind <- sample(seq_len(nrow(iris_sub)), size = smp_size)

train <- iris_sub[train_ind, ]
test <- iris_sub[-train_ind, ]

df_x  <- train[,(1:4)]

df_y <- as.data.frame(train[,5])

df_y <- as.data.frame(df_y$`train[, 5]` == "setosa")
```

**Hyperparameters
```{r}
learning_rate <- 0.5
epochs <- 10
```

**Various activation functions
```{r}
sigmoid <- function (z) {
  res <- 1 / (1 + exp(-1*z))
  return(res)
}

relu <- function (z) {
  res <- max(0,z)
  return(res)
}

tanh <- function (z) {
  res <- (exp(z) - exp(-1*z)) / (exp(z) + exp(-1*z))
}
```

**Regularization functions
```{r}
dropout <- function (layer_output) {
  z <- rbinom(nrow(layer_output), 1, 0.5)
  return(z * layer_output)
}
```

**The two parts of each node
```{r}
# IN: 1x1, 1x1, 1x1
# OUT: 1x1
node <- function (w, b, x, FUN = sigmoid) {
  z <- w * x + b
  # IN: 1x1
  # OUT: 1x1
  node_activation(z, FUN)
}

# IN: 1x1
# OUT: 1x1
node_activation <- function(z, FUN){
  y <- FUN(z)
  return(y)
}
```

**Layer of a NN
```{r}
# IN: 1x4, 1x1, 1x1
# OUT: 1x1
layer_node <- function (vals, FUN = sigmoid, weight_df, bias) {
  res <- c()
  for(i in 1:ncol(vals)) {
    # IN: 1x1, 1x1, 1x1
    # OUT: 1x1
    res[i] <- node(weight_df, bias, vals[,i], FUN)
  }
  
  return(sum(res))
}

# IN: 1x4, 5x1, 1x1
# OUT: 5x1
layer <- function (df, nodes, FUN = sigmoid, weight_df, bias) {
  res <- matrix(ncol=1, nrow=nodes)
  for(i in 1:nodes){
    # IN: 1x4, 1x1, 1x1
    # OUT: 1x1
    res[i,] <- layer_node(df, FUN, weight_df[i,], bias)
  }
  return(data.frame(res))
}
```

**Loss and Cost
```{r}
loss <- function(y_hat, y){
  res <- 1 / 2 * (y_hat - y)^2
  
  return(res)
}

cost <- function(l){
  l <- as.data.frame(l)
  res <- 1 / nrow(l) * colSums(l)
  return(res)
}
```

**Random Initialization
```{r}
rand_init <- function(K) {
  res <- data.frame(abs(rnorm(K)))
  return(res)
}
```

**Need to implement backprop
```{r}

```

**Update weights function
```{r}
update <- function(weights, change, learn_rate) {
  weights <- learn_rate*(weights - change)
}
```

**Implementation of a simple forward prop, with all observations and 5 epochs
```{r}
return <- as.data.frame(matrix(ncol=5, nrow=nrow(df_x)))

layer_0_weights <- rand_init(5)
layer_1_weights <- rand_init(10)
layer_2_weights <- rand_init(10)
layer_3_weights <- rand_init(1)
bias <- 0.05
for (j in 1:epochs) {
  for (i in 1:nrow(df_x)) {
    # IN: 1x4, 5x1, 1x1
    # OUT: 5x1
    layer_0 <- layer(df_x[i,], 5, sigmoid, layer_0_weights, bias)
    layer_0_t <- t(layer_0)
    # IN: 1x5, 10x1, 1x1
    # OUT: 10x1
    layer_1 <- layer(layer_0_t, 10, relu, layer_1_weights, bias)
    # IN: 10x1
    # OUT: 10x1
    layer_1_reg <- dropout(layer_1)
    layer_1_reg_t <- t(layer_1_reg)
    # IN: 1x1, 10x1, 1x1
    # OUT: 10x1
    layer_2 <- layer(layer_1_reg_t, 10, tanh, layer_2_weights, bias)
    # IN: 10x1
    # OUT: 10x1
    layer_2_reg <- dropout(layer_2)
    layer_2_reg_t <- t(layer_2_reg)
    # IN: 10x1, 1x1, 1x1
    # OUT: 1x1
    layer_3 <- layer(layer_2_reg_t, 1, sigmoid, layer_3_weights, bias)
    
    return[i,j] <- loss(layer_3, df_y[i,])
  }
  
  c <- cost(return[,j])
  
  layer_0_weights <- update(layer_0_weights, c, learning_rate)
  layer_1_weights <- update(layer_1_weights, c, learning_rate)
  layer_2_weights <- update(layer_2_weights, c, learning_rate)
  layer_3_weights <- update(layer_3_weights, c, learning_rate)
}
```





