---
title: "NN From Scratch"
output: html_notebook
---

**Get data set
```{r echo=TRUE}
data("iris")

iris_sub <- iris[ which(iris$Species == "setosa" | iris$Species == "virginica"), ]

df_x  <- iris_sub[,(1:4)]

df_y <- as.data.frame(iris_sub[,5])

df_y <- as.data.frame(df_y$`iris_sub[, 5]` == "setosa")
```

**Hyperparameters
```{r}
learning_rate <- 0.008
epochs <- 5
```

**Various activation funtions
```{r}
sigmoid <- function (z) {
  res <- 1 / (1 + exp(-1*z))
  return(res)
}

relu <- function (z) {
  res <- max(0,z)
  return(res)
}

tanh <- function (z) {
  res <- (exp(z) - exp(-1*z)) / (exp(z) + exp(-1*z))
}
```

**Regularization functions
```{r}
dropout <- function (layer_output) {
  z <- rbinom(nrow(layer_output), 1, 0.5)
  return(z * layer_output)
}
```

**The two parts of each node
```{r}
node <- function (w, b, x, FUN = sigmoid) {
  z <- w * x + b
  node_activation(z, FUN)
}

node_activation <- function(z, FUN){
  y <- FUN(z)
  return(y)
}
```

**Layer of a NN
```{r}
layer_node <- function (row, FUN = sigmoid, weights, bias) {
  res <- c()
  for(i in 1:length(row)) {
    res[i] <- node(weights[i], bias, as.double(row[i]), FUN)
  }
  
  return(sum(res))
}

##right here is where our output changes to a 1 column dataframe, will need to be transposed to list for future layers
layer <- function (df, FUN = sigmoid, weights, bias) {
  res <- matrix(ncol=1, nrow=nrow(df))
  for(i in 1:nrow(df)){
    res[i,] <- layer_node(df[i,], FUN, weights, bias)
  }
  return(data.frame(res))
}
```

**Subsequent layers after layer 0
```{r}
layer_after_0 <- function (df, nodes, FUN = sigmoid, weights, bias) {
  
  res <- matrix(ncol=nrow(df), nrow=nodes)
  for (i in 1:nodes) {
    df_t <- t(df)
    res[i,] <- df_t
  }
  return(layer(res, FUN, weights, bias))
}
```

**Loss and Cost
```{r}
loss <- function(y_hat, y){
  res <- 1 / 2 * (y_hat - y)^2
  
  return(res)
}

cost <- function(l){
  l <- as.data.frame(l)
  res <- 1 / nrow(l) * colSums(l)
  return(res)
}
```


**Build weights and bias df
```{r}
weights_bias_df <- function(df) {
  return(data.frame(x1 = numeric(nrow(df) + 1)))
}
```

**Random Initialization
```{r}
rand_init <- function(df, bias_start) {
  K <- nrow(df)
  res <- data.frame(rnorm(K))
  res[1,] <- bias_start
  return(res)
}
```

**Need to implement backprop
```{r}

```

**Update weights function
```{r}
update <- function(weights, change, learn_rate) {
  weights <- learn_rate*(weights - change)
}
```

**Implementation of a simple forward prop, with all observations and 5 epochs
```{r}
return <- as.data.frame(matrix(ncol=5, nrow=nrow(df_x)))

## This is set up so that we can learn these weights and biases once the backprop function is implemented, for now I am stripping out bias and weights into seperate dfs
weights_bias <- rand_init(weights_bias_df(df_x), 0.05)
bias <- weights_bias[1,]
weights <- weights_bias[2:nrow(weights_bias),]

for (j in epochs:1) {
  for (i in 1:nrow(df_x)) {
    layer_0 <- layer(df_x[i,], sigmoid, weights, bias)
  
    layer_1 <- layer_after_0(layer_0, 100, relu, weights, bias)

    layer_1_reg <- dropout(layer_1)

    layer_2 <- layer_after_0(layer_1_reg, 10, tanh, weights, bias)
    
    layer_2_reg <- dropout(layer_2)

    layer_3 <- layer_after_0(layer_2_reg, 1, sigmoid, weights, bias)
  
    return[i,j] <- loss(layer_3, df_y[i,])
  }
  
  c <- cost(return[,j])
  
  weights <- update(weights, c, learning_rate)
}
```





